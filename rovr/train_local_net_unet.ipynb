{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c6f7dc-074f-4850-bcb7-07b227bba8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cef8c3-fe56-4a29-983e-5b0f76745304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import lpips\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as Ft\n",
    "from torchvision.models.optical_flow import raft_small\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import lpips\n",
    "\n",
    "from video_ds import VideoDataset\n",
    "from local_net_unet import LocalNetworkUNet ##CHANGED FROM VIT VERSION\n",
    "from action_lstm import ActionLSTM\n",
    "from policy_net_1 import PolicyNetwork1\n",
    "from policy_net_2 import PolicyNetwork2\n",
    "from resnet_extractor import ResnetFeatureExtractor\n",
    "\n",
    "import random\n",
    "import time\n",
    "from itertools import cycle\n",
    "from pathlib import Path\n",
    "\n",
    "from GPUtil import showUtilization as gpu_usage, getAvailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c10824-63c6-4be9-ad54-5021becbb852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clamp(x, a, b): return x if a <= x <= b else (a if a > x else b)\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, video, orig_video):\n",
    "        self.video = video\n",
    "        self.orig_video = orig_video\n",
    "\n",
    "    def __len__(self):\n",
    "        return 500\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        l = idx % video.shape[0]\n",
    "        #pick two distinct numbers between 5 and 10\n",
    "        f = random.randint(5, 19)\n",
    "        n = clamp(random.randint(5, 10) * (2 * random.randint(0, 1) - 1), 0, 24)\n",
    "        m = n\n",
    "        while n == m:\n",
    "            m = clamp(random.randint(5, 10) * (2 * random.randint(0, 1) - 1), 0, 24)\n",
    "        image = self.video[l][f]\n",
    "        context1 = self.video[l][n]\n",
    "        context2 = self.video[l][m]\n",
    "        target = self.orig_video[l][m]\n",
    "        ## CHANGED FROM VIT\n",
    "        #context1 = F.interpolate(context1.unsqueeze(0), size = (128, 128), mode = \"bilinear\", align_corners = False).squeeze(0)\n",
    "        #context2 = F.interpolate(context2.unsqueeze(0), size = (128, 128), mode = \"bilinear\", align_corners = False).squeeze(0)\n",
    "        \n",
    "        return image, context1, context2, target\n",
    "\n",
    "def load_video_dataset(root_folder, num_workers):\n",
    "    dataset = VideoDataset(root_folder)\n",
    "    return DataLoader(dataset, batch_size=1, num_workers=num_workers, shuffle = True)\n",
    "\n",
    "def load_image_dataset(video, org_video, batch_size=32):\n",
    "    dataset = ImageDataset(video, org_video)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d21305-e588-4d8f-9051-d6bd37c51be5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:43<00:00,  9.66it/s]\n"
     ]
    }
   ],
   "source": [
    "l = list(tqdm(load_video_dataset(\"out/LQ\", num_workers = 32)))\n",
    "video, orig_video = torch.cat([t[0] for t in l]), torch.cat([t[1] for t in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1114feb-7c9c-4080-9e3e-2e8279834fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1 selected\n"
     ]
    }
   ],
   "source": [
    "local_net = LocalNetworkUNet()\n",
    "local_net_optimizer = torch.optim.Adam(local_net.parameters(), lr=1e-4)\n",
    "\n",
    "def parallel_and_device(model, device):\n",
    "    model = model.to(device) # shift model to device\n",
    "    return model\n",
    "\n",
    "# Select the GPU with the lowest utilization\n",
    "def get_available_device():\n",
    "    gpus_id = getAvailable(order = 'memory', maxLoad=0.5, maxMemory=0.5, includeNan=False, excludeID=[], excludeUUID=[])\n",
    "    if len(gpus_id) == 0:\n",
    "        print(\"No available GPU, will proceed with CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "    else:\n",
    "        print(f\"GPU {gpus_id[0]} selected\")\n",
    "        return torch.device(f\"cuda:{gpus_id[0]}\")\n",
    "\n",
    "device = get_available_device()\n",
    "local_net = parallel_and_device(local_net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0ce9b3-3c0f-44b5-b48c-2697cfaecd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "mse_loss_fn = torch.nn.MSELoss().to(device)\n",
    "lpips_loss_fn = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189b99fc-0cea-4887-97a6-2e8c7008d4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_ds = load_image_dataset(video, orig_video, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0117e01a-2c93-4c42-889d-50c98b05d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('runs') / 'local_net' / 'unet_mse' / time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())\n",
    "(path / 'checkpoints').mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0054b9-26f7-4618-a2e3-92d97d6ab4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=path, flush_secs=10)\n",
    "\n",
    "writer.add_graph(local_net, (next(iter(image_ds))[0].to(device), torch.stack(next(iter(image_ds))[1:3], dim=1).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1e4ad-3de0-4e1e-8da7-b543422fe7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2738it [13:57,  3.36it/s]"
     ]
    }
   ],
   "source": [
    "for i, (frame, context1, context2, target) in tqdm(enumerate(cycle(image_ds))):\n",
    "    local_net_optimizer.zero_grad()\n",
    "    image = frame.to(device), torch.stack([context1, context2], dim = 1).to(device)\n",
    "    y_hat = local_net(*image)\n",
    "    cuda_target = target.to(device)\n",
    "    mse_loss = mse_loss_fn(y_hat, cuda_target).mean()\n",
    "    writer.add_scalar('Loss/mse_loss', mse_loss.detach(), i)\n",
    "    lpips_loss = lpips_loss_fn(y_hat, cuda_target).mean()\n",
    "    writer.add_scalar('Loss/lpips_loss', lpips_loss.detach(), i)\n",
    "    mse_loss.backward()\n",
    "    #lpips_loss.backward()\n",
    "    local_net_optimizer.step()\n",
    "    if (i % 200) == 0:\n",
    "        display = torch.cat(list(torch.cat([frame, context1, context2, target, y_hat.cpu()], dim=3)[:10]), dim=1)\n",
    "        writer.add_image('Viz', display, i)\n",
    "    if (i % 2000) == 0:\n",
    "        torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': local_net.state_dict(),\n",
    "            'optimizer_state_dict': local_net_optimizer.state_dict(),\n",
    "            'mse_loss': mse_loss.detach(),\n",
    "            'lpips_loss': lpips_loss.detach(),\n",
    "            }, path / 'checkpoints' / f'{i}.pt')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
